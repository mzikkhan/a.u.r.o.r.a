---
title: "DATA534: Project Notebook"
author: "Mohammad Zaed Iqbal Khan"
date: "2026-01-26"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 21-Jan-2026

## Summary 

Today I focused on building a prototype pipeline for integrating macro-level contextual data (weather, economic indicators, and news signals) into a unified time-series format that can later be attached to revenue or performance datasets.

I experimented with three core components:

1) Weather Data Integration (Open-Meteo API):

I retrieved hourly temperature and precipitation data for a specified location (Canada) over a defined date range. I then performed data wrangling to aggregate hourly observations into daily features, including average temperature, maximum temperature, and total daily precipitation. This required parsing timestamps, creating daily groupings, and computing summary statistics.

2) Economic Indicators Integration (FRED API):

Using the fredr package, I fetched multiple macroeconomic indicators with varying frequencies (daily interest rates, weekly mortgage rates, monthly unemployment and CPI, and quarterly GDP and public debt). I reshaped the data into wide format and constructed a continuous daily timeline. To address mixed frequencies, I applied a Last Observation Carried Forward (LOCF) strategy so that lower-frequency variables could be aligned to daily observations.

3) News-Based Signals (GDELT API):

I experimented with GDELTâ€™s document API to retrieve timeline-based sentiment data for specific themes (e.g., protests, unrest, natural disasters) filtered by source country. This establishes a foundation for incorporating event-driven or sentiment-based external signals into the analysis.

## Role in the Larger Project

This work contributes directly to our group objective of building an R package that automates the attachment of macro-level contextual data to user-provided revenue datasets. The scripts developed today form the technical backbone for:

1. collecting heterogeneous external data,

2. standardizing them to a common daily time scale,

3. and preparing them for downstream visualization and trend analysis.

These components will later be wrapped into higher-level functions so analysts can enrich their revenue data with macro features using a single function call.

## Development Decisions

1. I chose Open-Meteo for weather data because it is free, does not require authentication, and provides hourly resolution suitable for daily aggregation.

2. I used FRED for economic indicators due to its reliability and wide coverage of macroeconomic variables.

3. For mixed-frequency economic data, I implemented Last Observation Carried Forward (LOCF) to align all series to daily granularity, as this reflects how macro indicators are typically treated in applied analytics.

4. I explored GDELT for news-based signals since it offers theme-based queries and timeline sentiment, making it suitable for capturing societal disruptions or major events.

5. I initially relied on base R aggregation functions to prototype quickly, with the intention of later refactoring into cleaner, reusable package functions.

```{r commit1}


# load packages

library(tidyverse)
library(fredr)
library(zoo)

## ------------------------------------------  WEATHER API ---------------------------------------------------

lat <- 51.5074 # location lattitude
lon <- -0.1278 # location longitude
start <- "2026-01-01" # start date
end   <- "2026-01-25" # end date

# 1. CALL API
url <- paste0("https://api.open-meteo.com/v1/forecast?",
              "latitude=", lat, "&longitude=", lon,
              "&start_date=", start, "&end_date=", end,
              "&hourly=temperature_2m,precipitation",
              "&timezone=auto",                        
              "&format=csv")

# 2. Read and Clean Data
weather_data <- read.csv(url, skip = 10, header = FALSE)
colnames(weather_data) <- c("datetime", "temp_c", "precip_mm")
weather_data$datetime <- as.POSIXct(weather_data$datetime, format="%Y-%m-%dT%H:%M")
weather_data$date <- as.Date(weather_data$datetime)

# 4. Calculate stats
daily_summary <- aggregate(cbind(temp_c, precip_mm) ~ date, 
                           data = weather_data, 
                           FUN = function(x) c(mean = mean(x), max = max(x), sum = sum(x)))

# 5. Clean up the messy matrix output from aggregate
daily_summary <- data.frame(
  date      = daily_summary$date,
  avg_temp  = round(daily_summary$temp_c[, "mean"], 1),
  max_temp  = daily_summary$temp_c[, "max"],
  tot_rain  = daily_summary$precip_mm[, "sum"]
)

print(daily_summary)

## ----------------------------------------  ECONOMIC API  --------------------------------------------------

# Set API key

fred_api_key <- '1f52a3bcb927a3a2423a9a2e78bd1261'
fredr_set_key(fred_api_key)

# 1. Define the parameters and their frequencies
indicators <- c(
  "Interest_Rate"     = "DFF",          # Daily
  "Mortgage_Rate"     = "MORTGAGE30US", # Weekly
  "Unemployment"      = "UNRATE",       # Monthly
  "CPI_Inflation"     = "CPIAUCSL",     # Monthly
  "GDP"               = "GDPC1",        # Quarterly
  "Public_Debt"       = "GFDEBTN"       # Quarterly
)

# 2. Fetch all data at once 
raw_data <- map_dfr(indicators, 
                    ~fredr(series_id = .x, observation_start = as.Date("2020-01-01")), 
                    .id = "metric")

# 3. Pivot to Wide Format 
daily_trend <- raw_data %>%
  select(date, metric, value) %>%
  pivot_wider(names_from = metric, values_from = value) %>%
  arrange(date)

# 4. The "Fill" Step (LOCF)
full_dates <- data.frame(date = seq(min(daily_trend$date), max(daily_trend$date), by="day"))

final_data <- full_dates %>%
  left_join(daily_trend, by = "date") %>%
  mutate(across(-date, ~na.locf(., na.rm = FALSE)))

# View the result
head(final_data)
tail(final_data)

## --------------------------------------------  NEWS API  ---------------------------------------------------

# -------------------------------------------  POLITICAL UNREST ----------------------------------------------

# 1. Define QUERY
query_text <- '(protest OR unrest) sourcecountry:Canada'

# 2. Call API
url <- paste0("https://api.gdeltproject.org/api/v2/doc/doc?",
              "query=", URLencode(query_text),
              "&mode=TimelineTone",
              "&timespan=1m",
              "&format=CSV")

# 3. Read the data 
raw_data <- read.csv(url, check.names = FALSE)

raw_data

# -----------------------------------------  NATURAL DISASTERS -----------------------------------------------

# 1. Define QUERY
query_text <- '(theme:NATURAL_DISASTER OR "disaster" OR "storm" OR "flood") sourcecountry:CA'

# 2. Call API
url <- paste0("https://api.gdeltproject.org/api/v2/doc/doc?",
              "query=", URLencode(query_text),
              "&mode=TimelineTone",
              "&timespan=1m",
              "&format=CSV")

# 3. Read the data 
raw_data <- try(read.csv(url, check.names = FALSE), silent = TRUE)

raw_data

```

Commit link: https://github.com/mzikkhan/a.u.r.o.r.a/commit/42e75ed3286d08483fc0b61e00bb52fd46501934#diff-e90f1dd576380e55c0157d832b265fcec51a1d050cdf6168a20ae0779a4e9e88R2-R4

# 26-Jan-2026








